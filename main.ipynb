{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original code by Philipp Muellauer\n",
    "<p> https://www.udemy.com/course/python-data-science-machine-learning-bootcamp/?couponCode=LEADERSALE24A </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 19:16:00.754866: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-23 19:16:01.357367: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "from time import strftime\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_OF_CLASSES = 10\n",
    "VALIDATION_SIZE = 10000\n",
    "IMAGE_WIDTH = 28\n",
    "IMAGE_HEIGHT = 28\n",
    "CHANNELS = 1\n",
    "TOTAL_INPUTS = IMAGE_WIDTH*IMAGE_HEIGHT*CHANNELS\n",
    "BATCH_SIZE = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_full = np.loadtxt('data/digit_xtrain.csv', delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.loadtxt('data/digit_xtest.csv', delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_full = np.loadtxt('data/digit_ytrain.csv', delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = np.loadtxt('data/digit_ytest.csv', delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.32941176 0.7254902\n",
      " 0.62352941 0.59215686 0.23529412 0.14117647 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.87058824 0.99607843 0.99607843 0.99607843\n",
      " 0.99607843 0.94509804 0.77647059 0.77647059 0.77647059 0.77647059\n",
      " 0.77647059 0.77647059 0.77647059 0.77647059 0.66666667 0.20392157\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.2627451  0.44705882 0.28235294 0.44705882 0.63921569 0.89019608\n",
      " 0.99607843 0.88235294 0.99607843 0.99607843 0.99607843 0.98039216\n",
      " 0.89803922 0.99607843 0.99607843 0.54901961 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.06666667 0.25882353 0.05490196\n",
      " 0.2627451  0.2627451  0.2627451  0.23137255 0.08235294 0.9254902\n",
      " 0.99607843 0.41568627 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.3254902  0.99215686 0.81960784 0.07058824\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.08627451\n",
      " 0.91372549 1.         0.3254902  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.50588235 0.99607843 0.93333333\n",
      " 0.17254902 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.23137255 0.97647059 0.99607843 0.24313725 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.52156863 0.99607843\n",
      " 0.73333333 0.01960784 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.03529412 0.80392157 0.97254902 0.22745098 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.49411765\n",
      " 0.99607843 0.71372549 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.29411765 0.98431373 0.94117647 0.22352941\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.0745098\n",
      " 0.86666667 0.99607843 0.65098039 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01176471 0.79607843 0.99607843 0.85882353\n",
      " 0.1372549  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.14901961 0.99607843 0.99607843 0.30196078 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.12156863 0.87843137 0.99607843\n",
      " 0.45098039 0.00392157 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.52156863 0.99607843 0.99607843 0.20392157 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.23921569 0.94901961\n",
      " 0.99607843 0.99607843 0.20392157 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.4745098  0.99607843 0.99607843 0.85882353\n",
      " 0.15686275 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.4745098  0.99607843 0.81176471 0.07058824 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Normalization (rescale)\n",
    "x_train_full = x_train_full / 255.0\n",
    "x_test = x_test / 255.0\n",
    "print(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot encoding\n",
    "y_train_full = np.eye(NUM_OF_CLASSES)[y_train_full]\n",
    "y_test = np.eye(NUM_OF_CLASSES)[y_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create subsets for traning and validation\n",
    "x_valid = x_train_full[:VALIDATION_SIZE]\n",
    "y_valid = y_train_full[:VALIDATION_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train_full[VALIDATION_SIZE:]\n",
    "y_train = y_train_full[VALIDATION_SIZE:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Create the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be able to create the graph\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, TOTAL_INPUTS], name='X')\n",
    "Y = tf.placeholder(tf.float32, shape=[None, NUM_OF_CLASSES], name='labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_of_epochs = 50\n",
    "learning_rate = 1e-3\n",
    "\n",
    "nodes_hidden1 = 512\n",
    "nodes_hidden2 = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Init parameters\n",
    "def setup_layer(input, weight_dim, bias_dim, name):\n",
    "    \n",
    "    with tf.name_scope(name):\n",
    "        init_weight = tf.truncated_normal(shape=weight_dim, stddev=0.1)\n",
    "        w = tf.Variable(initial_value=init_weight, name='W')\n",
    "\n",
    "        init_bias = tf.constant(value=0.0, shape=bias_dim)\n",
    "        b = tf.Variable(initial_value=init_bias, name='B')\n",
    "        \n",
    "        input_layer = tf.matmul(input, w) + b\n",
    "        \n",
    "        if name == 'out':\n",
    "            output_layer = tf.nn.softmax(input_layer)\n",
    "        else:\n",
    "            output_layer = tf.nn.relu(input_layer)\n",
    "        \n",
    "        tf.summary.histogram('weights', w)\n",
    "        tf.summary.histogram('biases', b)\n",
    "        \n",
    "        return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create layers\n",
    "layer_1 = setup_layer(X, weight_dim=[TOTAL_INPUTS, nodes_hidden1], \n",
    "                      bias_dim=[nodes_hidden1], name='layer_1')\n",
    "\n",
    "layer_dropout = tf.nn.dropout(layer_1, rate=0.8, name='dropout_layer')\n",
    "\n",
    "layer_2 = setup_layer(layer_dropout, weight_dim=[nodes_hidden1, nodes_hidden2], \n",
    "                      bias_dim=[nodes_hidden2], name='layer_2')\n",
    "\n",
    "output = setup_layer(layer_2, weight_dim=[nodes_hidden2, NUM_OF_CLASSES], \n",
    "                      bias_dim=[NUM_OF_CLASSES], name='out')\n",
    "\n",
    "model_name = f'N1-{nodes_hidden1}_Drop_N2-{nodes_hidden2}_LR-{learning_rate}_E-{num_of_epochs}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created directories!\n"
     ]
    }
   ],
   "source": [
    "# Folder for Tensorboard\n",
    "\n",
    "folder_name = f'{model_name}_Time-{strftime(\"%H:%M\")}'\n",
    "directory = os.path.join(\"logs/\", folder_name)\n",
    "\n",
    "try:\n",
    "    os.makedirs(directory)\n",
    "except OSError as exception:\n",
    "    print(exception)\n",
    "else:\n",
    "    print('Successfully created directories!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('loss_calc'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Elementwise comparison of Y and the predictions\n",
    "with tf.name_scope('accuracy_calc'):\n",
    "    model_prediction = tf.argmax(output, axis=1, name=\"prediction\")\n",
    "    correct_pred = tf.equal(model_prediction, tf.argmax(Y, axis=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('performance'):\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('cost', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('show_image'):\n",
    "    x_image = tf.reshape(X, [-1, 28, 28, 1])\n",
    "    tf.summary.image('image_input', x_image, max_outputs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_summary = tf.summary.merge_all()\n",
    "\n",
    "train_writer = tf.summary.FileWriter(directory + '/train')\n",
    "train_writer.add_graph(session.graph)\n",
    "\n",
    "validation_writer = tf.summary.FileWriter(directory + 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 19:16:03.768616: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n"
     ]
    }
   ],
   "source": [
    "# Add the variables\n",
    "init = tf.global_variables_initializer()\n",
    "session.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating batches\n",
    "num_of_samples = y_train.shape[0]\n",
    "num_of_iterations = int(num_of_samples/BATCH_SIZE)\n",
    "\n",
    "index_in_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(batch_size, data, labels):\n",
    "    \n",
    "    global num_of_samples\n",
    "    global index_in_epoch\n",
    "    \n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    if index_in_epoch > num_of_samples:\n",
    "        start = 0\n",
    "        index_in_epoch = batch_size\n",
    "    \n",
    "    end = index_in_epoch\n",
    "    \n",
    "    return data[start:end], labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 \t| Training Accuracy = 0.7960000038146973\n",
      "Epoch 1 \t| Training Accuracy = 0.902999997138977\n",
      "Epoch 2 \t| Training Accuracy = 0.9279999732971191\n",
      "Epoch 3 \t| Training Accuracy = 0.9399999976158142\n",
      "Epoch 4 \t| Training Accuracy = 0.9390000104904175\n",
      "Epoch 5 \t| Training Accuracy = 0.9459999799728394\n",
      "Epoch 6 \t| Training Accuracy = 0.9470000267028809\n",
      "Epoch 7 \t| Training Accuracy = 0.953000009059906\n",
      "Epoch 8 \t| Training Accuracy = 0.9559999704360962\n",
      "Epoch 9 \t| Training Accuracy = 0.953000009059906\n",
      "Epoch 10 \t| Training Accuracy = 0.9430000185966492\n",
      "Epoch 11 \t| Training Accuracy = 0.9570000171661377\n",
      "Epoch 12 \t| Training Accuracy = 0.9679999947547913\n",
      "Epoch 13 \t| Training Accuracy = 0.9629999995231628\n",
      "Epoch 14 \t| Training Accuracy = 0.9559999704360962\n",
      "Epoch 15 \t| Training Accuracy = 0.9629999995231628\n",
      "Epoch 16 \t| Training Accuracy = 0.9649999737739563\n",
      "Epoch 17 \t| Training Accuracy = 0.9660000205039978\n",
      "Epoch 18 \t| Training Accuracy = 0.9750000238418579\n",
      "Epoch 19 \t| Training Accuracy = 0.9599999785423279\n",
      "Epoch 20 \t| Training Accuracy = 0.9629999995231628\n",
      "Epoch 21 \t| Training Accuracy = 0.9660000205039978\n",
      "Epoch 22 \t| Training Accuracy = 0.9660000205039978\n",
      "Epoch 23 \t| Training Accuracy = 0.968999981880188\n",
      "Epoch 24 \t| Training Accuracy = 0.972000002861023\n",
      "Epoch 25 \t| Training Accuracy = 0.9710000157356262\n",
      "Epoch 26 \t| Training Accuracy = 0.9710000157356262\n",
      "Epoch 27 \t| Training Accuracy = 0.9700000286102295\n",
      "Epoch 28 \t| Training Accuracy = 0.9729999899864197\n",
      "Epoch 29 \t| Training Accuracy = 0.9729999899864197\n",
      "Epoch 30 \t| Training Accuracy = 0.9670000076293945\n",
      "Epoch 31 \t| Training Accuracy = 0.968999981880188\n",
      "Epoch 32 \t| Training Accuracy = 0.9710000157356262\n",
      "Epoch 33 \t| Training Accuracy = 0.9810000061988831\n",
      "Epoch 34 \t| Training Accuracy = 0.9710000157356262\n",
      "Epoch 35 \t| Training Accuracy = 0.9700000286102295\n",
      "Epoch 36 \t| Training Accuracy = 0.972000002861023\n",
      "Epoch 37 \t| Training Accuracy = 0.968999981880188\n",
      "Epoch 38 \t| Training Accuracy = 0.9789999723434448\n",
      "Epoch 39 \t| Training Accuracy = 0.9710000157356262\n",
      "Epoch 40 \t| Training Accuracy = 0.9700000286102295\n",
      "Epoch 41 \t| Training Accuracy = 0.972000002861023\n",
      "Epoch 42 \t| Training Accuracy = 0.9710000157356262\n",
      "Epoch 43 \t| Training Accuracy = 0.9750000238418579\n",
      "Epoch 44 \t| Training Accuracy = 0.9739999771118164\n",
      "Epoch 45 \t| Training Accuracy = 0.9769999980926514\n",
      "Epoch 46 \t| Training Accuracy = 0.9660000205039978\n",
      "Epoch 47 \t| Training Accuracy = 0.9760000109672546\n",
      "Epoch 48 \t| Training Accuracy = 0.9760000109672546\n",
      "Epoch 49 \t| Training Accuracy = 0.9739999771118164\n",
      "Done training!\n"
     ]
    }
   ],
   "source": [
    "# Loops\n",
    "for epoch in range(num_of_epochs):\n",
    "    \n",
    "    # Training\n",
    "    for i in range(num_of_iterations):\n",
    "        \n",
    "        batch_x, batch_y = next_batch(batch_size=BATCH_SIZE, data=x_train, labels=y_train)\n",
    "        \n",
    "        feed_dictionary = {X:batch_x, Y:batch_y}\n",
    "        \n",
    "        session.run(train_step, feed_dict=feed_dictionary)\n",
    "        \n",
    "    \n",
    "    s, batch_accuracy = session.run(fetches=[merged_summary, accuracy], feed_dict=feed_dictionary)\n",
    "        \n",
    "    train_writer.add_summary(s, epoch)\n",
    "    \n",
    "    print(f'Epoch {epoch} \\t| Training Accuracy = {batch_accuracy}')\n",
    "    \n",
    "    # Validation\n",
    "    summary = session.run(fetches=merged_summary, feed_dict={X:x_valid, Y:y_valid})\n",
    "    validation_writer.add_summary(summary, epoch)\n",
    "\n",
    "print('Done training!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_168984/3062419730.py:4: simple_save (from tensorflow.python.saved_model.simple_save) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This API was designed for TensorFlow v1. See https://www.tensorflow.org/guide/migrate for instructions on how to migrate your code to TensorFlow v2.\n",
      "WARNING:tensorflow:From /home/dori/.local/lib/python3.10/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:225: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This API was designed for TensorFlow v1. See https://www.tensorflow.org/guide/migrate for instructions on how to migrate your code to TensorFlow v2.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: saved_model/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "outputs = {\"accuracy_calc/prediction\": model_prediction}\n",
    "inputs = {\"X\": X}\n",
    "\n",
    "tf.saved_model.simple_save(session, \"saved_model\", inputs, outputs)\n",
    "\n",
    "# Make sure the session is running!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAYAAAByDd+UAAABIUlEQVR4Ae2U3Q2EIAzH6+XeHUFXcAI/tvJJ3UQ3cAKjmziCTsBdTZocaqFiwsPFJgZLgV/5Aw3U18CjvTyyNtQDPFW8aRrAz8UC6aUhwDRNGmcYBs23OaIzRBiB0jQFhBCIErGBKP6mH65d1xXGcYS+7yEMQ20YwikRLWByUFKbzfPMDsnzXNV1zcb3AZGkURSxOeMur5gIeGVB21jvQNhrfMVflkVlWaawlZr1lnIS0VNJkuRwe7k52O8ELIpiWxMvTFVVpvWPMakUOK4sy01CbF1NtEOSD9Nt2xZMz+S4Jb3HCPwFOcmnszaPLd4I67oO4jgG6eOWnCcLxHQQKjWqqVTU2Xmuh382T1JTjTtks7wR8F7aHuCN0zqf+v+SfgCAJwNqfagb/wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=28x28>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.open('data/test_img.png')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luminance mode --> Black & white\n",
    "greyscale_pic = img.convert('L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Swap blacks and whites\n",
    "img_array = np.invert(greyscale_pic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make it 1D\n",
    "test_img = img_array.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for test image is [2]\n"
     ]
    }
   ],
   "source": [
    "# Make a prediction\n",
    "prediction = session.run(fetches=tf.argmax(output, axis=1), feed_dict={X:[test_img]})\n",
    "print(f'Prediction for test image is {prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set is 95.72%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = session.run(fetches=accuracy, feed_dict={X:x_test, Y:y_test})\n",
    "print(f'Accuracy on test set is {test_accuracy:0.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Reset Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_writer.close()\n",
    "validation_writer.close()\n",
    "session.close()\n",
    "tf.reset_default_graph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
